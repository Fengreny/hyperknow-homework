import os
import json
from dotenv import load_dotenv
from google import genai
from google.genai import types

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ==========================================
# 0. åˆå§‹åŒ–ä¸æ•°æ®åŠ è½½
# ==========================================
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
MODEL_NAME = "gemini-2.5-flash" 

# åŠ è½½æœ¬åœ°æ•°æ®
with open("memory.json", "r", encoding="utf-8") as f:
    MEMORY_DB = json.load(f)

with open("file_metadata.json", "r", encoding="utf-8") as f:
    FILES_DB = json.load(f)


# ==========================================
# 1. å®šä¹‰å·¥å…· (Tools)
# ==========================================

def get_memory(category: str) -> str:
    """
    Tool 1: è®°å¿†è·å–å·¥å…·
    Reads the user's knowledge level from memory based on a category.

    Args:
        category: The category to check (e.g., 'astronomy', 'calculus').

    Returns:
        A string describing the user's knowledge level.
    """
    print(f"\nğŸ” [Tool: Memory] Reading memory for: {category}")
    levels = MEMORY_DB.get("knowledge_levels", {})
    # æ¨¡ç³ŠåŒ¹é…å¤„ç†
    for key, val in levels.items():
        if category.lower() in key.lower():
            return f"User Knowledge in {key}: Level={val['level']}. Details: {val['detailed_description']}"
    return "No specific memory found for this category."


def search_files(keywords: str) -> list[str]:
    """
    Tool 2: æœ¬åœ°æ–‡ä»¶é€‰æ‹©å·¥å…·
    Searches file metadata for relevant files based on keywords.

    Args:
        keywords: Search terms (e.g., 'sun orbit').

    Returns:
        A list of relevant file TITLES (strings).
    """
    print(f"\nğŸ“‚ [Tool: Files] Searching files for: {keywords}")
    hits = []
    kw_list = keywords.lower().split()

    for filename, data in FILES_DB.items():
        content = data["content"].lower()
        title = filename.lower()
        # ç®€å•çš„åŒ¹é…é€»è¾‘ï¼šåªè¦å‘½ä¸­ä¸€ä¸ªå…³é”®è¯å°±å…¥é€‰
        if any(k in content or k in title for k in kw_list):
            hits.append(filename)

    print(f"   -> Found {len(hits)} files.")
    return hits


def generate_reply_tool(instruction: str, file_titles: list[str], user_context: str) -> str:
    """
    Tool 3: å›å¤ç”Ÿæˆå·¥å…· (æµå¼è¾“å‡º)
    The FINAL tool to call. It generates the actual response to the user.

    Args:
        instruction: The specific instruction for the reply (e.g., "Summarize the content").
        file_titles: A list of filenames to include in the context.
        user_context: The user's knowledge level or other context found from memory.
    """
    print(f"\nâœï¸ [Tool: Reply] Generating reply with {len(file_titles)} files...")

    # 1. ç»„è£…ä¸Šä¸‹æ–‡ï¼šæ ¹æ® Director ä¼ æ¥çš„æ–‡ä»¶åï¼Œå»æ•°æ®åº“é‡ŒæŠŠçœŸæ­£çš„ Content æ‹¿å‡ºæ¥
    full_context = ""
    for title in file_titles:
        if title in FILES_DB:
            full_context += f"\n--- File: {title} ---\n{FILES_DB[title]['content']}\n"

    # 2. æ„å»º Prompt
    final_prompt = f"""
    You are a helpful tutor.

    [User Context]
    {user_context}

    [Reference Materials]
    {full_context}

    [Instruction]
    {instruction}

    Please provide a comprehensive answer based strictly on the materials above.
    Adjust the complexity to match the User Context.
    """

    # 3. è°ƒç”¨ç”Ÿæˆæ¨¡å‹ (è¿™é‡Œæ¨¡æ‹Ÿæµå¼è¾“å‡º)
    # æ³¨æ„ï¼šåœ¨ Tool å†…éƒ¨æˆ‘ä»¬å†æ¬¡è°ƒç”¨äº† client æ¥ç”Ÿæˆæœ€ç»ˆå†…å®¹
    try:
        response_stream = client.models.generate_content_stream(
            model=MODEL_NAME,
            contents=final_prompt
        )

        print("\n" + "=" * 20 + " Stream Start " + "=" * 20)
        final_text = ""
        for chunk in response_stream:
            print(chunk.text, end="", flush=True)
            final_text += chunk.text
        print("\n" + "=" * 20 + " Stream End " + "=" * 20 + "\n")

        return "Reply generated and streamed to user successfully."

    except Exception as e:
        return f"Error generating reply: {e}"


# ==========================================
# 2. Director Agent (ä¸»æ§)
# ==========================================

def run_hyperknow_agent():
    # 1. é…ç½® Director
    # è¿™é‡Œçš„ Director ä¸è´Ÿè´£å†™å†…å®¹ï¼Œåªè´Ÿè´£â€œè°ƒåº¦â€
    director_tools = [get_memory, search_files, generate_reply_tool]

    director_config = types.GenerateContentConfig(
        tools=director_tools,
        automatic_function_calling=types.AutomaticFunctionCallingConfig(
            disable=False,
            maximum_remote_calls=5
        ),
        system_instruction="""
        You are the 'Director Agent' of Hyperknow.
        Your goal is to fulfill the user's request by orchestrating tools.

        WORKFLOW:
        1. Analyze user request.
        2. Call `get_memory` to understand the user's level.
        3. Call `search_files` to find relevant file TITLES based on the topic.
        4. CRITICAL: You MUST call `generate_reply_tool` as the final step.
           - Pass the instruction.
           - Pass the list of file titles you found.
           - Pass the user context you found.

        DO NOT generate the final summary yourself. You are just the manager.
        """
    )

    # 2. å¯åŠ¨å¯¹è¯
    director_chat = client.chats.create(
        model=MODEL_NAME,
        config=director_config
    )

    user_query = "ç»™æˆ‘æ€»ç»“è¿™å­¦æœŸå¤©â½‚è¯¾ä¸Šçš„æ‰€æœ‰å†…å®¹"
    print(f"User: {user_query}")

    # 3. å‘é€æŒ‡ä»¤
    # SDK ä¼šè‡ªåŠ¨è¿›è¡Œï¼šDirector -> Memory -> Director -> Files -> Director -> Reply Tool
    response = director_chat.send_message(user_query)

    # 4. ç»“æŸ
    # å› ä¸º generate_reply_tool å·²ç»æ‰“å°äº†æµå¼å†…å®¹ï¼Œ
    # Director æœ€åçš„è¿”å›å€¼é€šå¸¸æ˜¯ "Reply generated..." è¿™ç§ç¡®è®¤ä¿¡æ¯
    print(f"\n[Director Final Status]: {response.text}")


if __name__ == "__main__":
    run_hyperknow_agent()
