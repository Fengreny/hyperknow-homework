import json
import os
import time
import google.generativeai as genai
from google.generativeai.types import content_types
from collections import abc
from dotenv import load_dotenv

# ====================== 0. é…ç½®ä¸åˆå§‹åŒ– ======================
load_dotenv()
genai.configure(api_key=os.environ["GEMINI_API_KEY"], transport='rest')


reply_model = genai.GenerativeModel("gemini-3-pro-preview")


# ====================== 1. å®šä¹‰å·¥å…·å‡½æ•° ======================


def memory_load_function(subject: str):
    """
    è¯»å–memory.jsonï¼ŒæŸ¥è¯¢ç”¨æˆ·æŒ‡å®šç§‘ç›®çš„çŸ¥è¯†æ°´å¹³ã€‚

    Args:
        subject: ç§‘ç›®åç§°ï¼Œä¾‹å¦‚ 'astronomy', 'calculus' ç­‰ã€‚
    """
    print(f"\nğŸ” [Tool] æ­£åœ¨æŸ¥è¯¢è®°å¿†åº“: {subject}")
    try:
        with open('memory.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        knowledge_levels = data.get("knowledge_levels", {})
        # æ¨¡ç³ŠåŒ¹é…å¤„ç†
        if subject.lower() in knowledge_levels:
            return knowledge_levels[subject.lower()]
        else:
            return {"error": f"æœªæ‰¾åˆ° {subject} çš„è®°å¿†ä¿¡æ¯"}
    except Exception as e:
        return {"error": f"è¯»å–è®°å¿†æ–‡ä»¶å‡ºé”™: {str(e)}"}


def select_files(query: str):
    """
    æ ¹æ®å…³é”®è¯æœç´¢æœ¬åœ°æ–‡ä»¶ï¼Œè¿”å›ç›¸å…³æ–‡ä»¶ååˆ—è¡¨ã€‚

    Args:
        query: æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ 'astronomy'ã€‚
    """
    print(f"\nğŸ“‚ [Tool] æ­£åœ¨æœç´¢æ–‡ä»¶å…³é”®è¯: {query}")
    found_files = []
    try:
        with open('file_metadata.json', 'r', encoding='utf-8') as f:
            file_data = json.load(f)
        for filename, info in file_data.items():
            content = info.get("content", "").lower()
            if query.lower() in content:
                found_files.append(filename)
        print(f"   âœ… æ‰¾åˆ° {len(found_files)} ä¸ªç›¸å…³æ–‡ä»¶")
        return found_files
    except Exception as e:
        return {"error": f"æœç´¢æ–‡ä»¶å‡ºé”™: {str(e)}"}


def reply_generator(file_titles: list[str], instruction: str):
    """
    æ ¹æ®æä¾›çš„æ–‡ä»¶åˆ—è¡¨å’ŒæŒ‡ä»¤ï¼Œæµå¼ç”Ÿæˆå›ç­”ã€‚

    Args:
        file_titles: éœ€è¦å‚è€ƒçš„æ–‡ä»¶ååˆ—è¡¨ã€‚
        instruction: ç”Ÿæˆå›å¤çš„å…·ä½“æŒ‡ä»¤ã€‚
    """
    print(f"\nğŸ“ [Tool] æ­£åœ¨è°ƒç”¨å›å¤ç”Ÿæˆå™¨...")
    print(f"   å‚è€ƒæ–‡ä»¶: {file_titles}")

    context_content = ""
    try:
        with open('file_metadata.json', 'r', encoding='utf-8') as f:
            all_files_data = json.load(f)
        for title in file_titles:
            if title in all_files_data:
                file_text = all_files_data[title]['content']
                context_content += f"\n--- æ–‡ä»¶å: {title} ---\n{file_text}\n"
    except Exception as e:
        return f"è¯»å–æ–‡ä»¶å†…å®¹å‡ºé”™: {str(e)}"

    # æ„å»ºæœ€ç»ˆç”Ÿæˆçš„ Prompt
    final_prompt = f"""
    ã€è§’è‰²ã€‘å¤©æ–‡è¯¾åŠ©æ•™
    ã€ç”¨æˆ·æŒ‡ä»¤ã€‘{instruction}
    ã€å‚è€ƒèµ„æ–™ã€‘{context_content if context_content else "æ— "}
    ã€è¦æ±‚ã€‘è¯·æ ¹æ®å‚è€ƒèµ„æ–™å’ŒæŒ‡ä»¤ï¼Œç”Ÿæˆè¯¦ç»†çš„æ€»ç»“ã€‚
    """

    print("\nğŸ’¡ [Stream] å¼€å§‹æµå¼è¾“å‡ºå›å¤: \n")
    # ä½¿ç”¨ç‹¬ç«‹çš„ reply_model è¿›è¡Œæµå¼ç”Ÿæˆï¼Œé¿å…å·¥å…·é€’å½’è°ƒç”¨
    response = reply_model.generate_content(final_prompt, stream=True)

    full_text = ""
    for chunk in response:
        if chunk.text:
            print(chunk.text, end="", flush=True)
            full_text += chunk.text
    print("\n")

    return "å›å¤å·²ç”Ÿæˆå®Œæ¯•ã€‚"


# å·¥å…·å­—å…¸ï¼Œç”¨äºæ‰‹åŠ¨æ‰§è¡Œ
functions = {
    'memory_load_function': memory_load_function,
    'select_files': select_files,
    'reply_generator': reply_generator
}

# ====================== 2. åˆå§‹åŒ– Director Agent ======================
# å°†å·¥å…·åˆ—è¡¨ä¼ ç»™æ¨¡å‹
tools_list = [memory_load_function, select_files, reply_generator]
director_model = genai.GenerativeModel("gemini-3-pro-preview", tools=tools_list)


# ====================== 3. æ ¸å¿ƒ Agent é€»è¾‘ ======================
class  directorAgent :
    def __init__(self):
        # å¼€å¯è‡ªåŠ¨å‡½æ•°è°ƒç”¨è®¾ä¸º Falseï¼Œæ‰‹åŠ¨æ§åˆ¶æµç¨‹
        self.chat = director_model.start_chat(enable_automatic_function_calling=False)

    def run(self, user_query: str):
        print(f"ğŸ‘¤ ç”¨æˆ·æŒ‡ä»¤ï¼š{user_query}")

        # å‘é€åˆå§‹æ¶ˆæ¯
        response = self.chat.send_message(user_query)

        # å¾ªç¯å¤„ç†ï¼šåªè¦æ¨¡å‹æƒ³è°ƒå‡½æ•°ï¼Œå°±ä¸€ç›´å¾ªç¯
        while True:
            part = response.parts[0]

            # 1. æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°è°ƒç”¨è¯·æ±‚ (Function Call)
            if part.function_call:
                fc = part.function_call
                func_name = fc.name
                func_args = dict(fc.args)

                print(f"\nğŸ¤– æ¨¡å‹è¯·æ±‚è°ƒç”¨: {func_name}")
                print(f"   å‚æ•°: {func_args}")

                # 2. æ‰§è¡Œå‡½æ•°
                if func_name in functions:
                    api_result = functions[func_name](**func_args)
                else:
                    api_result = {"error": "Function not found"}

                # 3. æ„å»ºå‡½æ•°å“åº” (Function Response)
                # Gemini éœ€è¦ç‰¹æ®Šçš„æ ¼å¼æŠŠç»“æœä¼ å›å»
                function_response_part = genai.protos.Part(
                    function_response=genai.protos.FunctionResponse(
                        name=func_name,
                        response={'result': api_result}
                    )
                )

                # 4. å°†ç»“æœå‘å›ç»™æ¨¡å‹ï¼Œè®©å®ƒå†³å®šä¸‹ä¸€æ­¥
                print(f"   ğŸ“¤ å°†ç»“æœå›ä¼ ç»™ Director Agent...")
                response = self.chat.send_message([function_response_part])

            # 5. å¦‚æœæ˜¯æ™®é€šæ–‡æœ¬ï¼Œè¯´æ˜ä»»åŠ¡ç»“æŸï¼ˆæˆ–è€…æ¨¡å‹åœ¨è‡ªè¨€è‡ªè¯­ï¼‰
            elif part.text:
                print(f"\nğŸ‰ Director Agent ä»»åŠ¡ç»“æŸ: {part.text}")
                break
            else:
                break


# ====================== è¿è¡Œæµ‹è¯• ======================
if __name__ == "__main__":
    agent = directorAgent()
    agent.run("ç»™æˆ‘æ€»ç»“è¿™å­¦æœŸå¤©æ–‡è¯¾ä¸Šçš„æ‰€æœ‰å†…å®¹")
